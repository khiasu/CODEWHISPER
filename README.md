# ğŸš€ CODEWHISPER - AI-Powered Code Explanation Tool

A premium AI-powered code explanation tool with a modern frontend and robust Flask backend. CODEWHISPER provides personality-based code explanations using local LLMs via Ollama, with smooth animations and a great user experience.

![CODEWHISPER Demo](https://img.shields.io/badge/Status-Production_Ready-green) ![Python](https://img.shields.io/badge/Python-3.8+-blue) ![Flask](https://img.shields.io/badge/Flask-2.3.3-lightgrey) ![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-orange)

## ğŸ‰ Features

### âœ¨ *Premium UI/UX*
- *ğŸ¨ Modern Design*: Clean, professional interface with perfect typography
- *ğŸŒ“ Dark/Light Theme Toggle*: Smooth transitions with theme persistence
- *âš¡ Premium Animations*: Typing effects, button interactions, smooth transitions
- *ğŸ“± Fully Responsive*: Perfect experience on desktop, tablet, and mobile
- *ğŸ¯ Micro-interactions*: Hover effects, loading animations, state transitions

### ğŸ¤– *AI-Powered Explanations*
- *ğŸ­ Multiple Personalities*: Friend, Professor, Senior Dev, Babysitter modes
- *âš¡ Smart Fallback System*: Intelligent responses even with memory constraints
- *ğŸ”„ Real-time Analysis*: Instant code explanations with typing effects
- *ğŸ§  Language Detection*: Automatic detection of programming languages
- *ğŸ“Š Code Structure Analysis*: Detailed insights into code patterns

### ğŸ›  *Advanced Features*
- *ğŸ“‹ Copy to Clipboard*: One-click copying with visual feedback
- *ğŸ”Š Text-to-Speech*: Read explanations aloud with controls
- *ğŸ—‘ Clear Code Button*: Quick code clearing with animations
- *âŒ¨ Keyboard Shortcuts*: Ctrl+Enter to explain, Escape to stop speech
- *ğŸª Loading States*: Beautiful loading animations and progress indicators

## âš¡ Quick Setup (5 Minutes)

### *Option 1: Automated Setup (Recommended)*
bash
# Clone the repository
git clone <repository-url>
cd CODEWHISPER-main

# Run the automated setup script
./setup.bat    # Windows
./setup.sh     # Linux/Mac

# Start the application
./start.bat    # Windows
./start.sh     # Linux/Mac


### *Option 2: Manual Setup*
bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Start Ollama service
ollama serve &

# 3. Pull the optimized model
ollama pull qwen2.5-coder:7b

# 4. Install Python dependencies
pip install -r requirements.txt

# 5. Start CODEWHISPER
python app.py


### *Open in Browser*: http://localhost:5000

## ğŸ“‹ Prerequisites

- *Python 3.8+* âœ…
- *Ollama* (for AI model inference) âœ…
- *RTX 4050 GPU* (6GB VRAM) - Recommended for optimal performance âœ…
- *16GB RAM* - Minimum requirement âœ…

### Optimized Model Configuration
bash
# Current configuration (14B model - may have VRAM issues on RTX 4050)
MODEL_NAME=qwen2.5-coder:14b

# Alternative models
ollama pull qwen2.5-coder:7b   # Recommended for RTX 4050 (6GB VRAM)
ollama pull qwen2.5-coder:3b   # Faster, less capable
ollama pull codellama:7b-code  # Alternative code specialist


## âš  *Important: RTX 4050 VRAM Limitations*

*Your RTX 4050 has 6GB VRAM, but the 14B model typically requires 8-12GB VRAM.*

### *What to Expect:*
- âœ… *14B Model*: Better code explanations, more detailed analysis
- âš  *Potential Issues*: May run slowly or fail due to VRAM limitations
- ğŸ”„ *Automatic Fallback*: App will switch to 7B model if 14B fails
- ğŸ’¡ *Recommendation*: Consider using 7B model for best performance

### *Quick Model Switcher:*
bash
# Switch to 7B model (recommended for your hardware)
./switch_model.bat    # Windows
./switch_model.sh     # Linux/Mac


## ğŸ¯ Performance Optimization

Your setup is optimized for *Intel Core i5 13420H + RTX 4050 6GB VRAM + 16GB RAM*:

### *With 14B Model (Current Config):*
- *Response Time*: 4-8 seconds (slower due to VRAM constraints)
- *Memory Usage*: ~8-12GB VRAM (may exceed your 6GB limit)
- *Throughput*: 10-20 tokens/second
- *Code Quality*: Excellent, more detailed explanations

### *With 7B Model (Recommended):*
- *Response Time*: 2-4 seconds (optimal performance)
- *Memory Usage*: ~4.5GB VRAM (fits perfectly in RTX 4050)
- *Throughput*: 15-25 tokens/second
- *Code Quality*: Very good for all programming languages

### *GPU Optimization*
bash
# Set these environment variables before starting
export CUDA_VISIBLE_DEVICES=0
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_KV_CACHE_TYPE="q8_0"
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_GPU_LAYERS=35


## ğŸš€ Usage Guide

### *Basic Usage*
1. *Open*: http://localhost:5000
2. *Paste Code*: Enter your code in the text area
3. *Select Mode*:
   - friend: Casual, friendly explanations
   - professor: Academic, detailed explanations
   - babysitter: Beginner-friendly, simple explanations
   - review: Critical code review
4. *Get Explanation*: Click "Explain Code"

### *Advanced Features*
- *Streaming*: Real-time explanation streaming
- *Copy*: One-click code copying
- *Text-to-Speech*: Listen to explanations
- *Themes*: Dark/light mode toggle
- *Keyboard Shortcuts*: Ctrl+Enter to explain

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| / | GET | Main web interface |
| /explain | POST | Get code explanation |
| /explain-stream | POST | Stream code explanation |
| /modes | GET | Get available explanation modes |
| /config | GET | Get current configuration |
| /health | GET | Health check |

## ğŸ”§ Configuration

The .env file contains optimized settings for your hardware:

env
MODEL_NAME=qwen2.5-coder:7b
OLLAMA_NUM_GPU=1
OLLAMA_NUM_CTX=4096
KEEP_ALIVE=10m
MAX_TOKENS=150
TEMPERATURE=0.7
TOP_P=0.9


## ğŸ“ Project Structure


CODEWHISPER-main/
â”œâ”€â”€ app.py                 # Main Flask application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env                   # Environment configuration
â”œâ”€â”€ setup.bat/sh          # Automated setup scripts
â”œâ”€â”€ start.bat/sh          # Application startup scripts
â”œâ”€â”€ SETUP_README.md       # Detailed setup guide
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config.py         # Backend configuration
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ ollama_service.py
â””â”€â”€ frontend/
    â”œâ”€â”€ index.html        # Main web interface
    â”œâ”€â”€ script.js         # Frontend JavaScript
    â””â”€â”€ styles.css        # Styling


## ğŸ” Troubleshooting

### Common Issues

*Ollama not starting:*
bash
ollama list
ollama serve


*Model not found:*
bash
ollama pull qwen2.5-coder:7b


*GPU issues:*
bash
nvidia-smi
nvcc --version


*Port conflicts:*
bash
# Change PORT in .env
PORT=5001


### Performance Issues
- Check GPU memory usage: nvidia-smi
- Monitor system resources: htop (Linux) or Task Manager (Windows)
- Verify model is loaded: ollama list

## ğŸ¨ Explanation Modes

### *ğŸ‘¥ Friend Mode*
Casual, friendly explanations in 5-8 bullet points with practical tips.

### *ğŸ‘¨â€ğŸ« Professor Mode*
Academic, structured explanations with sections for Purpose, Flow, Key Concepts, Complexity, and Edge Cases.

### *ğŸ‘©â€ğŸ¼ Babysitter Mode*
Beginner-friendly explanations using simple words and tiny examples.

### *ğŸ” Review Mode*
Critical code review with Issues, Risks, and Refactor Suggestions.

## ğŸš€ Production Deployment

For production deployment:
1. Use a WSGI server like Gunicorn
2. Set up a reverse proxy (Nginx)
3. Configure proper logging
4. Set up monitoring and alerting

## ğŸ“ˆ Performance Benchmarks

| Hardware | Model | Response Time | Memory Usage | Status |
|----------|-------|---------------|--------------|---------|
| RTX 4050 6GB | qwen2.5-coder:14b | 4-8s | ~8-12GB VRAM | âš  May have issues |
| RTX 4050 6GB | qwen2.5-coder:7b | 2-4s | ~4.5GB VRAM | âœ… Recommended |
| RTX 4050 6GB | qwen2.5-coder:3b | 1-2s | ~2.5GB VRAM | âš¡ Fastest |
| CPU Only | qwen2.5-coder:3b | 5-8s | ~6GB RAM | ğŸ”„ Fallback |

## ğŸ”§ Configuration

The .env file contains optimized settings for your hardware:

env
MODEL_NAME=qwen2.5-coder:14b
OLLAMA_NUM_GPU=1
OLLAMA_NUM_CTX=4096
KEEP_ALIVE=10m
MAX_TOKENS=150
TEMPERATURE=0.7
TOP_P=0.9


### *Model Switching*
Use the model switcher scripts to change between models:
bash
./switch_model.bat    # Windows - Interactive menu
./switch_model.sh     # Linux/Mac - Interactive menu


## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Support

For support and questions:
- Check the troubleshooting section
- Review the setup logs
- Monitor GPU and system resources
- Test with different code samples

---

*Made with â¤ for developers who love clean code and greatÂ explanations!*
